{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColabKobold GPU",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noxturnix/KoboldAI/blob/main/colab/GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX9y5koxa58q"
      },
      "source": [
        "# KoboldAI 0cc4m's fork (4bit support) on Google Colab\n",
        "\n",
        "This notebook allows you to download and use 4bit quantized models (GPTQ) on Google Colab.\n",
        "\n",
        "---\n",
        "# How to use\n",
        "\n",
        "0. If you are playing on a mobile device, tap the \"run\" button in the \"Tap this if you play on Mobile\" cell to prevent the system from killing this colab tab.\n",
        "1. Choose a GPTQ model in the \"Run this cell to download model\" cell. You can type a custom model name in the `Model` field, but make sure to [rename the model file](https://docs.alpindale.dev/static/koboldai-4bit-9.png) to the right name, then click the \"run\" button\n",
        "2. Click the \"run\" button in the \"Click this to start KoboldAI\" cell.\n",
        "3. After you get your KoboldAI URL, open it (assume you are using the new UI), click \"Load Model\", click \"Load a model from its directory\", and choose a model you downloaded.\n",
        "4. Enjoy! For prompting format, refer to the original model card of the model you selected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewkXkyiFP2Hq"
      },
      "source": [
        "#@title <-- Tap this if you play on Mobile { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldAI below (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <-- Run this cell to download model\n",
        "\n",
        "Model = \"Pygmalion 13B 4bit GPTQ\" #@param [\"Pygmalion 13B 4bit GPTQ\", \"Metharme 13B 4bit GPTQ\", \"Wizard Vicuna 13B 4bit GPTQ\", \"Wizard Vicuna 13B Uncensored 4bit GPTQ\", \"Pygmalion 7B 4bit GPTQ\", \"Metharme 7B 4bit GPTQ\", \"Wizard Vicuna 7B Uncensored 4bit GPTQ\"] {allow-input: true}\n",
        "rename_model_file_from = \"\" #@param {type:\"string\"}\n",
        "rename_model_file_to = \"\" #@param {type:\"string\"}\n",
        "download_to_google_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "from google.colab import drive\n",
        "if download_to_google_drive:\n",
        "  drive.mount('/content/drive/')\n",
        "else:\n",
        "  import os\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    os.mkdir(\"/content/drive\")\n",
        "  if not os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "    os.mkdir(\"/content/drive/MyDrive/\")\n",
        "\n",
        "if Model == \"Pygmalion 13B 4bit GPTQ\":\n",
        "  model_name = \"pygmalion-13b-4bit-128g\"\n",
        "  hf_url = f\"https://huggingface.co/notstoic/{model_name}\"\n",
        "elif Model == \"Metharme 13B 4bit GPTQ\":\n",
        "  model_name = \"Metharme-13b-4bit-GPTQ\"\n",
        "  hf_url = f\"https://huggingface.co/TehVenom/{model_name}\"\n",
        "  rename_model_file_from = \"model.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "elif Model == \"Wizard Vicuna 13B 4bit GPTQ\":\n",
        "  model_name = \"wizard-vicuna-13B-GPTQ\"\n",
        "  hf_url = f\"https://huggingface.co/TheBloke/{model_name}\"\n",
        "  rename_model_file_from = \"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "elif Model == \"Wizard Vicuna 13B Uncensored 4bit GPTQ\":\n",
        "  model_name = \"Wizard-Vicuna-13B-Uncensored-GPTQ\"\n",
        "  hf_url = f\"https://huggingface.co/TheBloke/{model_name}\"\n",
        "  rename_model_file_from = \"Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "elif Model == \"Pygmalion 7B 4bit GPTQ\":\n",
        "  model_name = \"Pygmalion-7b-4bit-GPTQ-Safetensors\"\n",
        "  hf_url = f\"https://huggingface.co/TehVenom/{model_name}\"\n",
        "  rename_model_file_from = \"Pygmalion-7B-GPTQ-4bit.act-order.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "elif Model == \"Metharme 7B 4bit GPTQ\":\n",
        "  model_name = \"Metharme-7b-4bit-GPTQ-Safetensors\"\n",
        "  hf_url = f\"https://huggingface.co/TehVenom/{model_name}\"\n",
        "  rename_model_file_from = \"Metharme-7B-GPTQ-4bit.act-order.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "elif Model == \"Wizard Vicuna 7B Uncensored 4bit GPTQ\":\n",
        "  model_name = \"Wizard-Vicuna-7B-Uncensored-GPTQ\"\n",
        "  hf_url = f\"https://huggingface.co/TheBloke/{model_name}\"\n",
        "  rename_model_file_from = \"Wizard-Vicuna-7B-Uncensored-GPTQ-4bit-128g.no-act-order.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "\n",
        "print(f\"Downloading {Model}...\")\n",
        "!mkdir -p /content/download_model && cd /content/download_model && git clone --single-branch --depth=1 $hf_url\n",
        "!rm -rf /content/download_model/$model_name/.git\n",
        "\n",
        "if rename_model_file_from and rename_model_file_to:\n",
        "  print(f\"Renaming {rename_model_file_from} to {rename_model_file_to}...\")\n",
        "  !cd /content/download_model/$model_name && mv $rename_model_file_from $rename_model_file_to\n",
        "\n",
        "if download_to_google_drive:\n",
        "  print(\"Uploading to Google Drive...\")\n",
        "!mkdir -p /content/drive/MyDrive/KoboldAI/models && mv /content/download_model/$model_name /content/drive/MyDrive/KoboldAI/models"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FJr0R36fLTNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVftocpwCoYw",
        "cellView": "form"
      },
      "source": [
        "#@title <b><-- Click this to start KoboldAI</b>\n",
        "\n",
        "Version = \"GPTQ\" #@param [\"GPTQ\"] {allow-input: false}\n",
        "Provider = \"Cloudflare\" #@param [\"Localtunnel\", \"Cloudflare\"]\n",
        "use_google_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "!nvidia-smi\n",
        "from google.colab import drive\n",
        "if use_google_drive:\n",
        "  drive.mount('/content/drive/')\n",
        "else:\n",
        "  import os\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    os.mkdir(\"/content/drive\")\n",
        "  if not os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "    os.mkdir(\"/content/drive/MyDrive/\")\n",
        "\n",
        "Revision = \"\"\n",
        "\n",
        "if Provider == \"Localtunnel\":\n",
        "  tunnel = \"--localtunnel yes\"\n",
        "else:\n",
        "  tunnel = \"\"\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Noxturnix/KoboldAI/main/colabkobold.sh -O - | bash /dev/stdin -g $Version $tunnel"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}