{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColabKobold GPU",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noxturnix/KoboldAI/blob/main/colab/GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX9y5koxa58q"
      },
      "source": [
        "# Welcome to KoboldAI on Google Colab, GPU Edition!\n",
        "KoboldAI is a powerful and easy way to use a variety of AI based text generation experiences. You can use it to write stories, blog posts, play a text adventure game, use it like a chatbot and more! In some cases it might even help you with an assignment or programming task (But always make sure the information the AI mentions is correct, it loves to make stuff up).\n",
        "\n",
        "For more information about KoboldAI check our our Github readme : https://github.com/KoboldAI/KoboldAI-Client/blob/main/readme.md\n",
        "\n",
        "---\n",
        "## How to load KoboldAI: Everything you need to know\n",
        "1. On a phone? First put your browser in desktop mode because of a Google Colab bug. Otherwise nothing will happen when you click the play button. Then tap the play button next to \"<-- Tap This if you play on Mobile\", you will see an audio player. Keep the audio player playing so Colab does not get shut down in the background.\n",
        "2. Select the desired model, you will find a description of all the available models further down the page.\n",
        "3. Click the play button next to \"<-- Select your model below and then click this to start KoboldAI\".\n",
        "4. Got a message saying no accelerator is available? Click cancel, and try again in a few minutes. If you do not manage to get a session when you frequently try again try at a different time of day, colab can be busy or your priority may have been lowered by frequent usage.\n",
        "5. After everything is done loading you will get a link that you can use to open KoboldAI. In case of Localtunnel you will also be warned that some people are abusing Localtunnel for phishing, once you acknowledge this warning you will be taken to KoboldAI's interface. If you picked Cloudflare and get a 1033 error refresh the error page after waiting one minute.\n",
        "\n",
        "---\n",
        "\n",
        "Further down the page you can find descriptions of the models, and tips to get the most out of your Google Colab experience.\n",
        "\n",
        "Make sure to keep this page open while you are using KoboldAI, and check back regularly to see if you got a Captcha. Failure to complete the captcha's in time can result in termination of your session or a lower priority towards the TPUs.\n",
        "\n",
        "Firefox users need to disable the enhanced tracking protection or use a different browser in order to be able to use Google Colab without errors (This is not something we can do anything about, the cookie blocker breaks the Google Drive integration because it uses different domains)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewkXkyiFP2Hq"
      },
      "source": [
        "#@title <-- Tap this if you play on Mobile { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldAI below (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <-- Run this cell to download model\n",
        "\n",
        "Model = \"Pygmalion 13B 4bit GPTQ\" #@param [\"Pygmalion 13B 4bit GPTQ\", \"Metharme 13B 4bit GPTQ\", \"Wizard Vicuna 13B 4bit GPTQ\", \"Wizard Vicuna 13B Uncensored 4bit GPTQ\", \"Pygmalion 7B 4bit GPTQ\", \"Metharme 7B 4bit GPTQ\", \"Wizard Vicuna 7B Uncensored 4bit GPTQ\"] {allow-input: true}\n",
        "rename_model_file_from = \"\" #@param {type:\"string\"}\n",
        "rename_model_file_to = \"\" #@param {type:\"string\"}\n",
        "download_to_google_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "from google.colab import drive\n",
        "if download_to_google_drive:\n",
        "  drive.mount('/content/drive/')\n",
        "else:\n",
        "  import os\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    os.mkdir(\"/content/drive\")\n",
        "  if not os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "    os.mkdir(\"/content/drive/MyDrive/\")\n",
        "\n",
        "if Model == \"Pygmalion 13B 4bit GPTQ\":\n",
        "  model_name = \"pygmalion-13b-4bit-128g\"\n",
        "  hf_url = f\"https://huggingface.co/notstoic/{model_name}\"\n",
        "elif Model == \"Metharme 13B 4bit GPTQ\":\n",
        "  model_name = \"Metharme-13b-4bit-GPTQ\"\n",
        "  hf_url = f\"https://huggingface.co/TehVenom/{model_name}\"\n",
        "  rename_model_file_from = \"model.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "elif Model == \"Wizard Vicuna 13B 4bit GPTQ\":\n",
        "  model_name = \"wizard-vicuna-13B-GPTQ\"\n",
        "  hf_url = f\"https://huggingface.co/TheBloke/{model_name}\"\n",
        "  rename_model_file_from = \"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "elif Model == \"Wizard Vicuna 13B Uncensored 4bit GPTQ\":\n",
        "  model_name = \"Wizard-Vicuna-13B-Uncensored-GPTQ\"\n",
        "  hf_url = f\"https://huggingface.co/TheBloke/{model_name}\"\n",
        "  rename_model_file_from = \"Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "elif Model == \"Pygmalion 7B 4bit GPTQ\":\n",
        "  model_name = \"Pygmalion-7b-4bit-GPTQ-Safetensors\"\n",
        "  hf_url = f\"https://huggingface.co/TehVenom/{model_name}\"\n",
        "  rename_model_file_from = \"Pygmalion-7B-GPTQ-4bit.act-order.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "elif Model == \"Metharme 7B 4bit GPTQ\":\n",
        "  model_name = \"Metharme-7b-4bit-GPTQ-Safetensors\"\n",
        "  hf_url = f\"https://huggingface.co/TehVenom/{model_name}\"\n",
        "  rename_model_file_from = \"Metharme-7B-GPTQ-4bit.act-order.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "elif Model == \"Wizard Vicuna 7B Uncensored 4bit GPTQ\":\n",
        "  model_name = \"Wizard-Vicuna-7B-Uncensored-GPTQ\"\n",
        "  hf_url = f\"https://huggingface.co/TheBloke/{model_name}\"\n",
        "  rename_model_file_from = \"Wizard-Vicuna-7B-Uncensored-GPTQ-4bit-128g.no-act-order.safetensors\"\n",
        "  rename_model_file_to = \"4bit-128g.safetensors\"\n",
        "\n",
        "print(f\"Downloading {Model}...\")\n",
        "!mkdir -p /content/download_model && cd /content/download_model && git clone --single-branch --depth=1 $hf_url\n",
        "!rm -rf /content/download_model/$model_name/.git\n",
        "\n",
        "if rename_model_file_from and rename_model_file_to:\n",
        "  print(f\"Renaming {rename_model_file_from} to {rename_model_file_to}...\")\n",
        "  !cd /content/download_model/$model_name && mv $rename_model_file_from $rename_model_file_to\n",
        "\n",
        "if download_to_google_drive:\n",
        "  print(\"Uploading to Google Drive...\")\n",
        "!mkdir -p /content/drive/MyDrive/KoboldAI/models && mv /content/download_model/$model_name /content/drive/MyDrive/KoboldAI/models"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FJr0R36fLTNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVftocpwCoYw",
        "cellView": "form"
      },
      "source": [
        "#@title <b><-- Click this to start KoboldAI</b>\n",
        "\n",
        "Version = \"GPTQ\" #@param [\"GPTQ\"] {allow-input: false}\n",
        "Provider = \"Cloudflare\" #@param [\"Localtunnel\", \"Cloudflare\"]\n",
        "use_google_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "!nvidia-smi\n",
        "from google.colab import drive\n",
        "if use_google_drive:\n",
        "  drive.mount('/content/drive/')\n",
        "else:\n",
        "  import os\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    os.mkdir(\"/content/drive\")\n",
        "  if not os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "    os.mkdir(\"/content/drive/MyDrive/\")\n",
        "\n",
        "Revision = \"\"\n",
        "\n",
        "if Provider == \"Localtunnel\":\n",
        "  tunnel = \"--localtunnel yes\"\n",
        "else:\n",
        "  tunnel = \"\"\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Noxturnix/KoboldAI/main/colabkobold.sh -O - | bash /dev/stdin -g $Version $tunnel"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
